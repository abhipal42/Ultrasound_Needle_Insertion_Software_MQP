{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "tensorflow version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import imutils \n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "tf.__version__\n",
    "import glob\n",
    "import imageio\n",
    "\n",
    "#check if tensorflow gpu is being used\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContours(img,imgContour):\n",
    "    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        \n",
    "        #Debugging statements\n",
    "        # if area > 1:\n",
    "        #     print(\"Area of contour is: {}\".format(area))\n",
    "        \n",
    "        areaMin = 3\n",
    "        areaMax = 35\n",
    "        if area > areaMin and area < areaMax:\n",
    "            cv2.drawContours(imgContour, cnt, -1, (255, 0, 255), 7)\n",
    "            peri = cv2.arcLength(cnt, True)\n",
    "            approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)\n",
    "            # print(len(approx))\n",
    "            x, y, w, h = cv2.boundingRect(approx)\n",
    "            cv2.rectangle(imgContour, (x, y), (x + w, y + h), (0, 255, 0), 5)\n",
    "\n",
    "            #compute center of contour\n",
    "            # M = cv2.moments(cnt)\n",
    "        \n",
    "            # if M[\"m00\"] != 0:\n",
    "            #     cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "            #     cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "            # else:\n",
    "            # # set values as what you need in the situation\n",
    "            #     cX, cY = 0, 0\n",
    "\n",
    "\t        # # draw the contour and center of the shape on the image\n",
    "            # cv2.circle(imgContour, (cX, cY), 7, (255, 255, 255), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V2.1 - Optical Flow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#videos to choose from\n",
    "NeedleViz_path1 = 'Data/edited data/102622_Water.mp4'\n",
    "NeedleViz_path2 = 'Data/edited data/102822_Water.mp4'\n",
    "NeedleViz_oilAndLatex = 'Data/edited data/oil and latex/capture_5_2022-11-12T16-56-03.mp4'\n",
    "NeedleViz_gelAndLatex = 'Data/edited data/ultrasound gel and latex/capture_4_2022-11-12T17-33-19.mp4'\n",
    "\n",
    "#control playback speed\n",
    "frame_rate = 30\n",
    "\n",
    "# vc = cv2.VideoCapture(0) #opens camera\n",
    "vc = cv2.VideoCapture(NeedleViz_path2)\n",
    "\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(history=100, varThreshold=40) #pretty good: (100,200)\n",
    "\n",
    "frameWidth = 440\n",
    "frameHeight = 440\n",
    "vc.set(3, frameWidth)\n",
    "vc.set(4, frameHeight)\n",
    "\n",
    "size = (frameWidth, frameHeight)\n",
    "\n",
    "#Preparing to create output videos\n",
    "image_lst = []\n",
    "\n",
    "\n",
    "if (vc.isOpened()== False): \n",
    "  print(\"Error opening video  file\")\n",
    "\n",
    "while(vc.isOpened()):\n",
    "    rval, frame = vc.read()\n",
    "    \n",
    "    if rval == True:\n",
    "\n",
    "\n",
    "        #Initial Frame preprocessing\n",
    "        resized_frame = cv2.resize(frame, (frameWidth,frameHeight))\n",
    "        resized_frame = cv2.cvtColor(resized_frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "\n",
    "        #Achieving desired region of interest within Raw Frame\n",
    "        ROI_frame = resized_frame[94:348, 166:275]\n",
    "         # ROI_frame = resized_frame[col_initial:col_final,row_initial:row_final]\n",
    "        blank_img = np.zeros_like(resized_frame)\n",
    "        imgContour = blank_img.copy()\n",
    "        imgContour2 = blank_img.copy()\n",
    "\n",
    "        x = 94 #initial column number\n",
    "        y = 166 #initial row number\n",
    "        for i in range(0, 254):\n",
    "          for j in range(0, 109):\n",
    "\n",
    "            if ROI_frame[i][j] != 0:\n",
    "              blank_img[x + i, y + j] = ROI_frame[i, j] \n",
    "\n",
    "\n",
    "        fgmask = fgbg.apply(blank_img)\n",
    "        _, fgmask = cv2.threshold(fgmask, 254, 255, cv2.THRESH_BINARY) #want only white pixels\n",
    "      \n",
    "\n",
    "        #Applying Basic Filters\n",
    "        #############################################################\n",
    "        imgBlur = cv2.GaussianBlur(blank_img,(7,7), 1)\n",
    "        # erode = cv2.erode(imgBlur, None, iterations=2)\n",
    "        # dilate = cv2.dilate(erode, None, iterations=2)\n",
    "        gaussian = 255*imgBlur+100\n",
    "        # median = cv2.medianBlur(contrast_imgBlur, 7)\n",
    "        ##############################################################\n",
    "\n",
    "        #Applying Background Subtraction (detecting regions of motion within frame)\n",
    "        fgmaskV2 = fgbg.apply(imgBlur)\n",
    "        _, fgmaskV2 = cv2.threshold(fgmaskV2, 254, 255, cv2.THRESH_BINARY) #want only white pixels\n",
    "\n",
    "        maskV2 = cv2.erode(fgmaskV2, None, iterations=2) \n",
    "        maskV2 = cv2.dilate(maskV2, None, iterations=2)\n",
    "\n",
    "        #Applying Contour detection (detectiong only regions past certain size)\n",
    "        getContours(fgmaskV2,imgContour)\n",
    "        getContours(fgmask,imgContour2)\n",
    "\n",
    "        #Applying trajectory/velocity/object tracking --> (detecting which pixels moves the most within the video --> grab coordinates once found) --> CURRENTLY NOT WORKING\n",
    "        \n",
    "        #Overlaying segmentations onto B-mode image\n",
    "        #############################################################################################\n",
    "        fgmaskV2_color = cv2.applyColorMap(imgContour, cv2.COLORMAP_INFERNO)\n",
    "        resized_frame_revert = cv2.cvtColor(resized_frame, cv2.COLOR_GRAY2RGB)\n",
    "        overlay = cv2.addWeighted(resized_frame_revert, 0.5, fgmaskV2_color, 0.5, 1.0)\n",
    "        # cv2.imshow(\"Bmode Overlay\", overlay)\n",
    "        ###########################################################################################\n",
    "\n",
    "        # Debugging Statements\n",
    "        cv2.imshow('normal frame', resized_frame)\n",
    "        cv2.imshow('FGmask', fgmask)\n",
    "        # cv2.imshow('Basic Filters', imgBlur)\n",
    "        # cv2.imshow('Canny', imgCanny_initial)\n",
    "        cv2.imshow('FGmaskV2', fgmaskV2)\n",
    "        # cv2.imshow('FGmaskV2_withfilters', maskV2)\n",
    "        cv2.imshow('Contour', imgContour)\n",
    "        cv2.imshow('Contour2', imgContour2)\n",
    "        # cv2.imshow('trajectory', trajectory_frame)\n",
    "        # print(\"raw image shape = {}\".format(resized_frame.shape))\n",
    "        # print(\"segmented image shape = {}\".format(fgmaskV2_color.shape))\n",
    "        # print(\"raw image shape (reverted)= {}\".format(resized_frame_revert.shape))\n",
    "        \n",
    "        #Saving comparison frames as gif \n",
    "        # resized_frame = cv2.cvtColor(resized_frame, cv2.COLOR_GRAY2BGR)\n",
    "        # overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)\n",
    "        # stack = np.hstack((resized_frame, overlay))\n",
    "        # # cv2.imshow(\"stacked\", stack)\n",
    "        # image_lst.append(stack)\n",
    "\n",
    "\n",
    "        # Press Q on keyboard to  exit\n",
    "        if cv2.waitKey(frame_rate) & 0xFF == ord('q'): #original waitkey is 25\n",
    "            break\n",
    "    \n",
    "    #Break out of loop if video is done\n",
    "    else:\n",
    "        break  \n",
    "\n",
    "vc.release() #Release the video capture object\n",
    "\n",
    "# Close window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Video as GIF\n",
    "# imageio.mimsave('Outputs/V1_video.gif', image_lst, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V2.2 - Object Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#videos to choose from\n",
    "NeedleViz_path1 = 'Data/edited data/102622_Water.mp4'\n",
    "NeedleViz_path2 = 'Data/edited data/102822_Water.mp4'\n",
    "NeedleViz_oilAndLatex = 'Data/edited data/oil and latex/capture_5_2022-11-12T16-56-03.mp4'\n",
    "NeedleViz_gelAndLatex = 'Data/edited data/ultrasound gel and latex/capture_4_2022-11-12T17-33-19.mp4'\n",
    "\n",
    "#control playback speed\n",
    "frame_rate = 30\n",
    "\n",
    "# vc = cv2.VideoCapture(0) #opens camera\n",
    "vc = cv2.VideoCapture(NeedleViz_path2)\n",
    "\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(history=100, varThreshold=40) #pretty good: (100,200)\n",
    "\n",
    "frameWidth = 440\n",
    "frameHeight = 440\n",
    "vc.set(3, frameWidth)\n",
    "vc.set(4, frameHeight)\n",
    "\n",
    "size = (frameWidth, frameHeight)\n",
    "\n",
    "#Preparing to create output videos\n",
    "image_lst = []\n",
    "\n",
    "\n",
    "if (vc.isOpened()== False): \n",
    "  print(\"Error opening video  file\")\n",
    "\n",
    "while(vc.isOpened()):\n",
    "    rval, frame = vc.read()\n",
    "    \n",
    "    if rval == True:\n",
    "\n",
    "\n",
    "        #Initial Frame preprocessing\n",
    "        resized_frame = cv2.resize(frame, (frameWidth,frameHeight))\n",
    "        resized_gray_frame = cv2.cvtColor(resized_frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "\n",
    "        #Achieving desired region of interest within Raw Frame\n",
    "        ROI_frame = resized_gray_frame[94:348, 166:275]\n",
    "         # ROI_frame = resized_frame[col_initial:col_final,row_initial:row_final]\n",
    "        blank_img = np.zeros_like(resized_gray_frame)\n",
    "\n",
    "        x = 94 #initial column number\n",
    "        y = 166 #initial row number\n",
    "        for i in range(0, 254):\n",
    "          for j in range(0, 109):\n",
    "\n",
    "            if ROI_frame[i][j] != 0:\n",
    "              blank_img[x + i, y + j] = ROI_frame[i, j] \n",
    "\n",
    "\n",
    "        # imgContour = blank_img.copy()\n",
    "        imgContour2 = resized_frame.copy()\n",
    "\n",
    "        fgmask = fgbg.apply(blank_img)\n",
    "        _, fgmask = cv2.threshold(fgmask, 254, 255, cv2.THRESH_BINARY) #want only white pixels\n",
    "\n",
    "        #Applying Basic Filters\n",
    "        #############################################################\n",
    "        imgBlur = cv2.GaussianBlur(blank_img,(7,7), 1)\n",
    "        # erode = cv2.erode(imgBlur, None, iterations=2)\n",
    "        # dilate = cv2.dilate(erode, None, iterations=2)\n",
    "        gaussian = 255*imgBlur+100\n",
    "        # median = cv2.medianBlur(contrast_imgBlur, 7)\n",
    "        ##############################################################\n",
    "\n",
    "        #Applying Background Subtraction (detecting regions of motion within frame)\n",
    "        fgmaskV2 = fgbg.apply(imgBlur)\n",
    "        _, fgmaskV2 = cv2.threshold(fgmaskV2, 254, 255, cv2.THRESH_BINARY) #want only white pixels\n",
    "\n",
    "        maskV2 = cv2.erode(fgmaskV2, None, iterations=2) \n",
    "        maskV2 = cv2.dilate(maskV2, None, iterations=2)\n",
    "\n",
    "        #Applying Contour detection (detectiong only regions past certain size)\n",
    "        getContours(fgmaskV2,imgContour)\n",
    "        getContours(fgmask,imgContour2)\n",
    "\n",
    "        #Applying trajectory/velocity/object tracking --> (detecting which pixels moves the most within the video --> grab coordinates once found) --> CURRENTLY NOT WORKING\n",
    "        \n",
    "        #Overlaying segmentations onto B-mode image\n",
    "        #############################################################################################\n",
    "        fgmaskV2_color = cv2.applyColorMap(imgContour, cv2.COLORMAP_INFERNO)\n",
    "        resized_frame_revert = cv2.cvtColor(resized_gray_frame, cv2.COLOR_GRAY2RGB)\n",
    "        overlay = cv2.addWeighted(resized_frame_revert, 0.5, fgmaskV2_color, 0.5, 1.0)\n",
    "        # cv2.imshow(\"Bmode Overlay\", overlay)\n",
    "        ###########################################################################################\n",
    "\n",
    "        # Debugging Statements\n",
    "        cv2.imshow('normal frame', resized_gray_frame)\n",
    "        cv2.imshow('FGmask', fgmask)\n",
    "        # cv2.imshow('Basic Filters', imgBlur)\n",
    "        # cv2.imshow('Canny', imgCanny_initial)\n",
    "        cv2.imshow('FGmaskV2', fgmaskV2)\n",
    "        # cv2.imshow('FGmaskV2_withfilters', maskV2)\n",
    "        cv2.imshow('Contour', imgContour)\n",
    "        cv2.imshow('Contour2', imgContour2)\n",
    "        # cv2.imshow('trajectory', trajectory_frame)\n",
    "        # print(\"raw image shape = {}\".format(resized_frame.shape))\n",
    "        # print(\"segmented image shape = {}\".format(fgmaskV2_color.shape))\n",
    "        # print(\"raw image shape (reverted)= {}\".format(resized_frame_revert.shape))\n",
    "        \n",
    "        #Saving comparison frames as gif \n",
    "        # resized_frame = cv2.cvtColor(resized_frame, cv2.COLOR_GRAY2BGR)\n",
    "        # overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)\n",
    "        # stack = np.hstack((resized_frame, overlay))\n",
    "        # # cv2.imshow(\"stacked\", stack)\n",
    "        # image_lst.append(stack)\n",
    "\n",
    "\n",
    "        # Press Q on keyboard to  exit\n",
    "        if cv2.waitKey(frame_rate) & 0xFF == ord('q'): #original waitkey is 25\n",
    "            break\n",
    "    \n",
    "    #Break out of loop if video is done\n",
    "    else:\n",
    "        break  \n",
    "\n",
    "vc.release() #Release the video capture object\n",
    "\n",
    "# Close window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V2.3 - Microbubble Tracking (Developed by OpenAI ChatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#videos to choose from\n",
    "NeedleViz_path1 = 'Data/edited data/102622_Water.mp4'\n",
    "NeedleViz_path2 = 'Data/edited data/102822_Water.mp4'\n",
    "NeedleViz_oilAndLatex = 'Data/edited data/oil and latex/capture_5_2022-11-12T16-56-03.mp4'\n",
    "NeedleViz_gelAndLatex = 'Data/edited data/ultrasound gel and latex/capture_4_2022-11-12T17-33-19.mp4'\n",
    "\n",
    "# Load the ultrasound video\n",
    "video = cv2.VideoCapture(NeedleViz_path2)\n",
    "\n",
    "# Set up the background subtractor\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Initialize a list to store the microbubble positions\n",
    "microbubble_positions = []\n",
    "\n",
    "# Loop through each frame of the video\n",
    "while True:\n",
    "  # Read the current frame\n",
    "  success, frame = video.read()\n",
    "\n",
    "  # If the frame was successfully read\n",
    "  if success:\n",
    "    # Apply the background subtractor to the frame\n",
    "    fg_mask = bg_subtractor.apply(frame)\n",
    "\n",
    "    # Use thresholding to isolate the microbubbles in the foreground mask\n",
    "    _, thresh = cv2.threshold(fg_mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find the contours of the microbubbles\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Loop through each contour\n",
    "    for c in contours:\n",
    "      # Calculate the centroid of the contour\n",
    "      M = cv2.moments(c)\n",
    "      if M[\"m00\"] != 0:\n",
    "        cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "        # Store the centroid position in the microbubble positions list\n",
    "        microbubble_positions.append((cx, cy))\n",
    "    \n",
    "    # Loop through the microbubble positions\n",
    "    for (x, y) in microbubble_positions:\n",
    "      # Draw a circle at the position of the microbubble\n",
    "      cv2.circle(frame, (x, y), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Display the frame with the tracked microbubbles\n",
    "    cv2.imshow(\"Tracked Microbubbles\", frame)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # If the user pressed the 'q' key, stop the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "      break\n",
    "\n",
    "  # If the frame was not successfully read, stop the loop\n",
    "  else:\n",
    "    break\n",
    "\n",
    "# When everything is done, release the video capture object\n",
    "video.release()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('clarius')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9962539669d5dbe9522d46f218cca01fd3b7ad23018b1c5959dcfe17c72706ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
